{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Agent\n",
    "\n",
    "By themselves language models can't take actions - they just output text. A big case for LangChain is creating agents. Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be. The results of those actions can then be fed back into the agent and it determines whether more actions are needed or it is okay to finish. \n",
    "\n",
    "In this tutorial we will build an agent that can interact with multiple tools: one being a local database, the other being a search engine. You will be able to ask this agent questions, watch it call tools, and have a conversation with it.\n",
    "\n",
    "## Concepts\n",
    "\n",
    "- Using [language models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), in particular their ability to call tools,\n",
    "- Creating a [Retriever](https://python.langchain.com/v0.2/docs/concepts/#retrievers) to expose specific information to our agent,\n",
    "- Using a [Search Tool](https://python.langchain.com/v0.2/docs/concepts/#tools) to look up things online,\n",
    "- Using [LangGraph Agents](https://python.langchain.com/v0.2/docs/concepts/#agents) which use an LLM to think about what to do and then execute upon that,\n",
    "- Debugging and tracing your application using [LangSmith](https://python.langchain.com/v0.2/docs/concepts/#langsmith)\n",
    "  - *We will probably not be doing this as we're not paying for LangSmith access*\n",
    "  - I think this is possible with *LangServe Playground* if we really need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We're expected to be using a jupyter notebook for this (and it seems like all) tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "We first need to create the tools we want to use. We will use two tools today: [Tavily](https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/) (to search online) and then a retriever over a local index that we will create.\n",
    "\n",
    "### [Tavily](https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/)\n",
    "\n",
    "**OH LOOK ANOTHER PAYWALL IN THE TUTORIALS. THIS IS GETTING OLD!** It appears as though we also have the ability to use duckduckgo as a search engine. It's free, so we will be using that instead. We might have to find a better way to do web scraping, but for now this will suffice.\n",
    "\n",
    "```ps\n",
    "pip install -U duckduckgo-search\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "search.invoke(\"what is the weather in SF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "\n",
    "We will also create a retriever over some data of our own. For a deeper explanation of each step here see [this tutorial](https://python.langchain.com/v0.2/docs/tutorials/rag/).\n",
    "\n",
    "In order to run the following script on windows we need to install faiss. It is only available as a CPU version on windows.\n",
    "\n",
    "```ps\n",
    "pip install faiss-cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, OllamaEmbeddings(model=\"mistral:v0.3\"))\n",
    "retriever = vector.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"How to upload a dataset?\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have populated our index that we will do retrieval over we can easily turn it into a tool (the format needed for an agent to properly use it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "Now that we have created both we can create a list of tools that we will use downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolList = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Language Models\n",
    "\n",
    "Next let's learn how to use a language model to call tools. LangChain supports many language models that you can use interchangeably - select the one that you want to use below (we'll be using a local model).\n",
    "\n",
    "As of yet they don't support passing tools to a local model. We have to run two separate instances of the model to accomplish what should only take one. This is definitely an area of future improvement.\n",
    "\n",
    "```ps\n",
    "pip install langchain_experimental\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "chatModel = ChatOllama(model=\"mistral:v0.3\", base_url=\"http://localhost:11434\")\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "functionModel = OllamaFunctions(model=\"mistral:v0.3\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the language model by passing in a list of messages. The response is a `content` string by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = chatModel.invoke([HumanMessage(content=\"Hi!\")])\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see what it is like to enable this model to do tool caling. In order to enable that we use `.bind_tools` to give the language model knowledge of these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = functionModel.bind_tools(toolList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the model. Let's first call it with a normal message and see how it responds. We can look at both the `content` field as well as the `tool_calls` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the tools are in the wrong format. I'll need to figure out how to properly format them to work with `ollama_functions`; both `chatollama` and `ollama_functions` aren't compatible with the way the tools are defined here. We'll have to find custom tooling to get this to work.\n",
    "\n",
    "[ollama_functions docs](https://api.python.langchain.com/en/latest/llms/langchain_experimental.llms.ollama_functions.OllamaFunctions.html)  \n",
    "[ollama_functions docs2](https://python.langchain.com/v0.1/docs/integrations/chat/ollama_functions/)  \n",
    "[BaseTool docs](https://api.python.langchain.com/en/latest/tools/langchain_core.tools.BaseTool.html#langchain_core.tools.BaseTool)  \n",
    "\n",
    "After doing some further digging it seems like the ollama api and the way langchain calls it aren't compatible with tool use. If I want to do this I'll have to go lower and start with the `ollama` api directly. This is a bit of a bummer, but it's not the end of the world. I'll have to do some more digging to figure out how to get this to work.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollamalangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
